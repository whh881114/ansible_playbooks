---
k8s_api_server: apiserver.k8s.freedom.org
prometheus_version: 2.25.2
prometheus_binary_local_dir: ''
prometheus_skip_install: false

prometheus_config_dir: /etc/prometheus
prometheus_db_dir: /var/lib/prometheus
prometheus_read_only_dirs: []

prometheus_web_listen_address: "0.0.0.0:9090"
prometheus_web_external_url: ''
# See https://github.com/prometheus/exporter-toolkit/blob/master/docs/web-configuration.md
prometheus_web_config:
  tls_server_config: {}
  http_server_config: {}
  basic_auth_users: {}

prometheus_storage_retention: "30d"
# Available since Prometheus 2.7.0
# [EXPERIMENTAL] Maximum number of bytes that can be stored for blocks. Units
# supported: KB, MB, GB, TB, PB.
prometheus_storage_retention_size: "0"

prometheus_config_flags_extra: {}
# prometheus_config_flags_extra:
#   storage.tsdb.retention: 15d
#   alertmanager.timeout: 10s

prometheus_alertmanager_config:
  - scheme: http
    path_prefix: alertmanager/
    static_configs:
      - targets:
          - "127.0.0.1:9093"
# prometheus_alertmanager_config:
#   - scheme: https
#     path_prefix: alertmanager/
#     basic_auth:
#       username: user
#       password: pass
#     static_configs:
#       - targets: ["127.0.0.1:9093"]
#     proxy_url: "127.0.0.2"

prometheus_alert_relabel_configs: []
# prometheus_alert_relabel_configs:
#   - action: labeldrop
#     regex: replica

prometheus_global:
  scrape_interval: 15s
  scrape_timeout: 10s
  evaluation_interval: 15s

prometheus_remote_write: []
# prometheus_remote_write:
#   - url: https://dev.kausal.co/prom/push
#     basic_auth:
#       password: FOO

prometheus_remote_read: []
# prometheus_remote_read:
#   - url: https://demo.cloudalchemy.org:9201/read
#     basic_auth:
#       password: FOO

prometheus_external_labels:
  environment: "{{ ansible_fqdn | default(ansible_host) | default(inventory_hostname) }}"

prometheus_targets: {}
#  node:
#    - targets:
#        - localhost:9100
#      labels:
#        env: test

prometheus_scrape_configs:
  - job_name: "prometheus"
    metrics_path: "{{ prometheus_metrics_path }}"
    static_configs:
      - targets:
          - "{{ ansible_fqdn | default(ansible_host) | default('localhost') }}:9090"
  - job_name: "alertmanager"
    static_configs:
      - targets:
          - "{{ ansible_fqdn | default(ansible_host) | default('localhost') }}:9093"
  - job_name: "pushgateway"
    honor_labels: true
    static_configs:
      - targets:
          - "{{ ansible_fqdn | default(ansible_host) | default('localhost') }}:9091"
# 私有云中的consul-server是在k8s集群中搭建，取消此配置。20210418。
#  - job_name: "consul_server_node_exporter"
#    static_configs:
#      - targets:
#          - "server01.consul.freedom.org:9100"
#          - "server02.consul.freedom.org:9100"
#          - "server03.consul.freedom.org:9100"
#  - job_name: "consul_sd_consul_exporter"
#    consul_sd_configs:
#      - server: "localhost:8500"
#        services:
#          - "consul"
#    relabel_configs:
#      - source_labels:
#          - __meta_consul_node
#        target_label: hostname
#      - source_labels:
#          - __address__
#        target_label: __address__
#        regex: (.*)\:(\d+)
#        replacement: "${1}:9107"
  - job_name: "consul_sd_node_exporter"
    consul_sd_configs:
      - server: "localhost:8500"  # 地址可以写成一个，但是如果consul-server是集群模式，那这个支持不了三个，所以还是写本地地址最好。
        services:
          - "node-exporter"
    relabel_configs:
      - source_labels:
          - __meta_consul_node
        target_label: hostname
  - job_name: "consul_sd_haproxy_exporter"
    consul_sd_configs:
      - server: "localhost:8500"
        services:
          - "haproxy-exporter"
    relabel_configs:
      - source_labels:
          - __meta_consul_node
        target_label: hostname
  - job_name: kubernetes-apiservers
    scheme: https
    metrics_path: /metrics
    tls_config:
      insecure_skip_verify: true
    bearer_token_file: "{{ prometheus_config_dir }}/k8s-prometheus.token"
    kubernetes_sd_configs:
      - role: endpoints
        api_server: "https://{{ k8s_api_server }}:6443"
        bearer_token_file: "{{ prometheus_config_dir }}/k8s-prometheus.token"
        tls_config:
          insecure_skip_verify: true
    relabel_configs:
      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        separator: ;
        regex: default;kubernetes;https
        replacement: $1
        action: keep
      - separator: ;
        regex: (.*)
        target_label: __address__
        replacement: "{{ k8s_api_server }}:6443"
        action: replace
  - job_name: kubernetes-node-kubelet
    scheme: https
    tls_config:
      insecure_skip_verify: true
    bearer_token_file: "{{ prometheus_config_dir }}/k8s-prometheus.token"
    kubernetes_sd_configs:
    - role: node
      api_server: "https://{{ k8s_api_server }}:6443"
      tls_config:
        insecure_skip_verify: true
      bearer_token_file: "{{ prometheus_config_dir }}/k8s-prometheus.token"
    relabel_configs:
    - target_label: __address__
      replacement: "{{ k8s_api_server }}:6443"
    - source_labels: [__meta_kubernetes_node_name]
      regex: (.+)
      target_label: __metrics_path__
      replacement: /api/v1/nodes/${1}:10250/proxy/metrics
    - action: labelmap
      regex: __meta_kubernetes_service_label_(.+)
    - source_labels: [__meta_kubernetes_namespace]
      action: replace
      target_label: kubernetes_namespace
    - source_labels: [__meta_kubernetes_service_name]
      action: replace
      target_label: service_name
  - job_name: kubernetes-node-cadvisor
    scheme: https
    tls_config:
      insecure_skip_verify: true
    bearer_token_file: "{{ prometheus_config_dir }}/k8s-prometheus.token"
    kubernetes_sd_configs:
    - role: node
      api_server: "https://{{ k8s_api_server }}:6443"
      tls_config:
        insecure_skip_verify: true
      bearer_token_file: "{{ prometheus_config_dir }}/k8s-prometheus.token"
    relabel_configs:
    - target_label: __address__
      replacement: "{{ k8s_api_server }}:6443"
    - source_labels: [__meta_kubernetes_node_name]
      regex: (.+)
      target_label: __metrics_path__
      replacement: /api/v1/nodes/${1}:10250/proxy/metrics/cadvisor
    - action: labelmap
      regex: __meta_kubernetes_service_label_(.+)
    - source_labels: [__meta_kubernetes_namespace]
      action: replace
      target_label: kubernetes_namespace
    - source_labels: [__meta_kubernetes_service_name]
      action: replace
      target_label: service_name




#  - job_name: kubernetes-node-exporter
#    metrics_path: /metrics
#    scheme: https
#    bearer_token_file: "{{ prometheus_config_dir }}/k8s-prometheus.token"
#    tls_config:
#      insecure_skip_verify: true
#    kubernetes_sd_configs:
#      - api_server: "https://{{ k8s_api_server }}:6443"
#        role: node
#        bearer_token_file: "{{ prometheus_config_dir }}/k8s-prometheus.token"
#        tls_config:
#          insecure_skip_verify: true
#    relabel_configs:
#      - separator: ;
#        regex: __meta_kubernetes_node_label_(.+)
#        replacement: $1
#        action: labelmap
#      - separator: ;
#        regex: (.*)
#        target_label: __address__
#        replacement: "{{ k8s_api_server }}:6443"
#        action: replace
#      - source_labels: [__meta_kubernetes_node_name]
#        separator: ;
#        regex: (.+)
#        target_label: __metrics_path__
#        replacement: /api/v1/nodes/${1}:9100/proxy/metrics
#        action: replace
#  - job_name: kubernetes-node-kubelet
#    metrics_path: /metrics
#    scheme: https
#    kubernetes_sd_configs:
#    - api_server: "https://{{ k8s_api_server }}:6443"
#      role: node
#      bearer_token_file: "{{ prometheus_config_dir }}/k8s-prometheus.token"
#      tls_config:
#        insecure_skip_verify: true
#    bearer_token_file: "{{ prometheus_config_dir }}/k8s-prometheus.token"
#    tls_config:
#      insecure_skip_verify: true
#    relabel_configs:
#    - separator: ;
#      regex: __meta_kubernetes_node_label_(.+)
#      replacement: $1
#      action: labelmap
#    - separator: ;
#      regex: (.*)
#      target_label: __address__
#      replacement: "{{ k8s_api_server }}:6443"
#      action: replace
#    - source_labels: [__meta_kubernetes_node_name]
#      separator: ;
#      regex: (.+)
#      target_label: __metrics_path__
#      replacement: /api/v1/nodes/${1}:10255/proxy/metrics
#      action: replace
#  - job_name: kubernetes-cadvisor
#    metrics_path: /metrics
#    scheme: https
#    kubernetes_sd_configs:
#    - api_server: "https://{{ k8s_api_server }}:6443"
#      role: node
#      bearer_token_file: "{{ prometheus_config_dir }}/k8s-prometheus.token"
#      tls_config:
#        insecure_skip_verify: true
#    bearer_token_file: "{{ prometheus_config_dir }}/k8s-prometheus.token"
#    tls_config:
#      insecure_skip_verify: true
#    relabel_configs:
#    - separator: ;
#      regex: __meta_kubernetes_node_label_(.+)
#      replacement: $1
#      action: labelmap
#    - separator: ;
#      regex: (.*)
#      target_label: __address__
#      replacement: "{{ k8s_api_server }}:6443"
#      action: replace
#    - source_labels: [__meta_kubernetes_node_name]
#      separator: ;
#      regex: (.+)
#      target_label: __metrics_path__
#      replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
#      action: replace
#  - job_name: 'kubernetes-pods'
#    scheme: https
#    kubernetes_sd_configs:
#    - api_server: "https://{{ k8s_api_server }}:6443"
#      role: pod
#      bearer_token_file: "{{ prometheus_config_dir }}/k8s-prometheus.token"
#      tls_config:
#        insecure_skip_verify: true
#    tls_config:
#      insecure_skip_verify: true
#    bearer_token_file: "{{ prometheus_config_dir }}/k8s-prometheus.token"
#    relabel_configs:
#    - source_labels: [__meta_kubernetes_namespace]
#      action: replace
#      target_label: kubernetes_namespace
#    - source_labels: [__meta_kubernetes_pod_name]
#      action: replace
#      target_label: kubernetes_pod_name
#    - action: labelmap
#      regex: __meta_kubernetes_pod_label_(.+)
#    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
#      action: replace
#      target_label: __metrics_path__
#      regex: (.+)
#    - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
#      action: replace
#      regex: ([^:]+)(?::\d+)?;(\d+)
#      replacement: $1:$2
#      target_label: __address__
#    - source_labels: [__address__]
#      separator: ;
#      regex: '.*:(.*)'
#      target_label: __pod_port__
#      replacement: $1
#      action: replace
#    - source_labels: [__meta_kubernetes_namespace,__meta_kubernetes_pod_name, __pod_port__]
#      separator: ;
#      regex: (.*);(.*);(.*)
#      target_label: __metrics_path__
#      replacement: /api/v1/namespaces/$1/pods/$2:$3/proxy/metrics
#      action: replace
#    - source_labels: [__address__]
#      separator: ;
#      regex: (.*)
#      target_label: __address__
#      replacement: "{{ k8s_api_server }}:6443"
#      action: replace
#  - job_name: 'kubernetes-services'
#    scheme: https
#    kubernetes_sd_configs:
#    - api_server: "https://{{ k8s_api_server }}:6443"
#      role: service
#      bearer_token_file: "{{ prometheus_config_dir }}/k8s-prometheus.token"
#      tls_config:
#        insecure_skip_verify: true
#    tls_config:
#      insecure_skip_verify: true
#    bearer_token_file: "{{ prometheus_config_dir }}/k8s-prometheus.token"
#    relabel_configs:
#    - separator: ;
#      regex: __meta_kubernetes_service_label_(.+)
#      replacement: $1
#      action: labelmap
#    - source_labels: [__address__]
#      separator: ;
#      regex: '.*:(.*)'
#      target_label: __service_port__
#      replacement: $1
#      action: replace
#    - source_labels: [__meta_kubernetes_namespace,__meta_kubernetes_service_name, __service_port__]
#      separator: ;
#      regex: (.*);(.*);(.*)
#      target_label: __metrics_path__
#      replacement: /api/v1/namespaces/$1/services/$2:$3/proxy/metrics
#      action: replace
#    - source_labels: [__address__]
#      separator: ;
#      regex: (.*)
#      target_label: __address__
#      replacement: "{{ k8s_api_server }}:6443"
#      action: replace
#  - job_name: kubernetes-endpoints
#    scheme: https
#    metrics_path: /metrics
#    kubernetes_sd_configs:
#    - role: endpoints
#    - api_server: "https://{{ k8s_api_server }}:6443"
#      role: endpoints
#      bearer_token_file: "{{ prometheus_config_dir }}/k8s-prometheus.token"
#      tls_config:
#        insecure_skip_verify: true
#    bearer_token_file: "{{ prometheus_config_dir }}/k8s-prometheus.token"
#    tls_config:
#      insecure_skip_verify: true
#    relabel_configs:
#    - separator: ;
#      regex: __meta_kubernetes_service_label_(.+)
#      replacement: $1
#      action: labelmap
#    - source_labels: [__meta_kubernetes_namespace]
#      separator: ;
#      regex: (.*)
#      target_label: kubernetes_namespace
#      replacement: $1
#      action: replace
#    - source_labels: [__meta_kubernetes_endpoints_name]
#      separator: ;
#      regex: (.*)
#      target_label: kubernetes_endpoint_name
#      replacement: $1
#      action: replace
#    - source_labels: [__address__]
#      separator: ;
#      regex: '.*:(.*)'
#      target_label: __service_port__
#      replacement: $1
#      action: replace
#    - source_labels: [__meta_kubernetes_namespace,__meta_kubernetes_endpoint_address_target_name, __service_port__]
#      separator: ;
#      regex: (.*);(.*);(.*)
#      target_label: __metrics_path__
#      replacement: /api/v1/namespaces/$1/pods/$2:$3/proxy/metrics
#      action: replace
#    - source_labels: [__address__]
#      separator: ;
#      regex: (.*)
#      target_label: __address__
#      replacement: "{{ k8s_api_server }}:6443"
#      action: replace


# 汪浩浩配置，根据定义不同的job_name来进行主机分类从而在grafana上面显示。consul_sd发现的机器数量过多，不分组则不易显示。
file_sd_services:
  - k8s-master
  - k8s-worker


# Alternative config file name, searched in ansible templates path.
prometheus_config_file: 'prometheus.yml.j2'

prometheus_alert_rules_files:
  - prometheus/rules/*.rules

prometheus_static_targets_files:
  - prometheus/targets/*.yml
  - prometheus/targets/*.json

prometheus_alert_rules:
  - alert: Watchdog
    expr: vector(1)
    for: 10m
    labels:
      severity: warning
    annotations:
      description: "This is an alert meant to ensure that the entire alerting pipeline is functional.\nThis alert is always firing, therefore it should always be firing in Alertmanager\nand always fire against a receiver. There are integrations with various notification\nmechanisms that send a notification when this alert is not firing. For example the\n\"DeadMansSnitch\" integration in PagerDuty."
      summary: 'Ensure entire alerting pipeline is functional'
  - alert: InstanceDown
    expr: 'up == 0'
    for: 5m
    labels:
      severity: critical
    annotations:
      description: '{% raw %}{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.{% endraw %}'
      summary: '{% raw %}Instance {{ $labels.instance }} down{% endraw %}'
  - alert: RebootRequired
    expr: 'node_reboot_required > 0'
    labels:
      severity: warning
    annotations:
      description: '{% raw %}{{ $labels.instance }} requires a reboot.{% endraw %}'
      summary: '{% raw %}Instance {{ $labels.instance }} - reboot required{% endraw %}'
  - alert: NodeFilesystemSpaceFillingUp
    annotations:
      description: '{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left and is filling up.{% endraw %}'
      summary: 'Filesystem is predicted to run out of space within the next 24 hours.'
    expr: "(\n  node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 40\nand\n  predict_linear(node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"}[6h], 24*60*60) < 0\nand\n  node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\n"
    for: 1h
    labels:
      severity: warning
  - alert: NodeFilesystemSpaceFillingUp
    annotations:
      description: '{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left and is filling up fast.{% endraw %}'
      summary: 'Filesystem is predicted to run out of space within the next 4 hours.'
    expr: "(\n  node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 20\nand\n  predict_linear(node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"}[6h], 4*60*60) < 0\nand\n  node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\n"
    for: 1h
    labels:
      severity: critical
  - alert: NodeFilesystemAlmostOutOfSpace
    annotations:
      description: '{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left.{% endraw %}'
      summary: 'Filesystem has less than 5% space left.'
    expr: "(\n  node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 5\nand\n  node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\n"
    for: 1h
    labels:
      severity: warning
  - alert: NodeFilesystemAlmostOutOfSpace
    annotations:
      description: '{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available space left.{% endraw %}'
      summary: 'Filesystem has less than 3% space left.'
    expr: "(\n  node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 3\nand\n  node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\n"
    for: 1h
    labels:
      severity: critical
  - alert: NodeFilesystemFilesFillingUp
    annotations:
      description: '{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left and is filling up.{% endraw %}'
      summary: 'Filesystem is predicted to run out of inodes within the next 24 hours.'
    expr: "(\n  node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 40\nand\n  predict_linear(node_filesystem_files_free{job=\"node\",fstype!=\"\"}[6h], 24*60*60) < 0\nand\n  node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\n"
    for: 1h
    labels:
      severity: warning
  - alert: NodeFilesystemFilesFillingUp
    annotations:
      description: '{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left and is filling up fast.{% endraw %}'
      summary: 'Filesystem is predicted to run out of inodes within the next 4 hours.'
    expr: "(\n  node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 20\nand\n  predict_linear(node_filesystem_files_free{job=\"node\",fstype!=\"\"}[6h], 4*60*60) < 0\nand\n  node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\n"
    for: 1h
    labels:
      severity: critical
  - alert: NodeFilesystemAlmostOutOfFiles
    annotations:
      description: '{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left.{% endraw %}'
      summary: 'Filesystem has less than 5% inodes left.'
    expr: "(\n  node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 5\nand\n  node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\n"
    for: 1h
    labels:
      severity: warning
  - alert: NodeFilesystemAlmostOutOfFiles
    annotations:
      description: '{% raw %}Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left.{% endraw %}'
      summary: 'Filesystem has less than 3% inodes left.'
    expr: "(\n  node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 3\nand\n  node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\n"
    for: 1h
    labels:
      severity: critical
  - alert: NodeNetworkReceiveErrs
    annotations:
      description: '{% raw %}{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf "%.0f" $value }} receive errors in the last two minutes.{% endraw %}'
      summary: 'Network interface is reporting many receive errors.'
    expr: "increase(node_network_receive_errs_total[2m]) > 10\n"
    for: 1h
    labels:
      severity: warning
  - alert: NodeNetworkTransmitErrs
    annotations:
      description: '{% raw %}{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf "%.0f" $value }} transmit errors in the last two minutes.{% endraw %}'
      summary: 'Network interface is reporting many transmit errors.'
    expr: "increase(node_network_transmit_errs_total[2m]) > 10\n"
    for: 1h
    labels:
      severity: warning
  - alert: NodeHighNumberConntrackEntriesUsed
    annotations:
      description: '{% raw %}{{ $value | humanizePercentage }} of conntrack entries are used{% endraw %}'
      summary: 'Number of conntrack are getting close to the limit'
    expr: "(node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75\n"
    labels:
      severity: warning
  - alert: NodeClockSkewDetected
    annotations:
      message: '{% raw %}Clock on {{ $labels.instance }} is out of sync by more than 300s. Ensure NTP is configured correctly on this host.{% endraw %}'
      summary: 'Clock skew detected.'
    expr: "(\n  node_timex_offset_seconds > 0.05\nand\n  deriv(node_timex_offset_seconds[5m]) >= 0\n)\nor\n(\n  node_timex_offset_seconds < -0.05\nand\n  deriv(node_timex_offset_seconds[5m]) <= 0\n)\n"
    for: 10m
    labels:
      severity: warning
  - alert: NodeClockNotSynchronising
    annotations:
      message: '{% raw %}Clock on {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host.{% endraw %}'
      summary: 'Clock not synchronising.'
    expr: "min_over_time(node_timex_sync_status[5m]) == 0\n"
    for: 10m
    labels:
      severity: warning
  - alert: HostCPU
    expr: 100 * (1 - avg(irate(node_cpu_seconds_total{mode="idle"}[2m])) by(instance)) > 10
    for: 5m
    labels:
      serverity: high
    annotations:
      summary: "High CPU Usage Detected"
      message: "{% raw %}{{$labels.instance}}: CPU usage is {{$value}}, above 10%.{% endraw %}"
  - alert: HostMemory
    expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 95
    for: 5m
    labels:
      serverity: middle
    annotations:
      summary: "High Memory Usage Detected"
      message: "{% raw %}{{$labels.instance}}: Memory Usage i{{ $value }}, above 95%.{% endraw %}"